{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import uproot as uproot\n",
    "import uproot3 as uproot3\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trio refers to the fact that I'm constraining M_A, NormCCMEC, and Lambda all at the same time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data files:\n",
    "# /exp/uboone/data/users/lcoopert/LEE/LEEana_xs_3D_reproduction/wiener_svd/merge_xs_data.root\n",
    "# /exp/uboone/data/users/lcoopert/LEE/LEEana_xs_3D_reproduction/wiener_svd/wiener_data.root\n",
    "\n",
    "# Asimov files:\n",
    "# /exp/uboone/data/users/lcoopert/LEE/LEEana_xs_3D_reproduction/wiener_svd/merge_xs_asimov.root # THIS FILE DOESN'T EXIST, but it should be the same prediction histograms as real data basically\n",
    "# /exp/uboone/data/users/lcoopert/LEE/LEEana_xs_3D_reproduction/wiener_svd/wiener_asimov.root\n",
    "\n",
    "# NuWro Fake Data files:\n",
    "# /exp/uboone/data/users/lcoopert/LEE/LEEana_xs_3D_fakedata_nuwro/wiener_svd/merge_xs.root\n",
    "# /exp/uboone/data/users/lcoopert/LEE/LEEana_xs_3D_fakedata_nuwro/wiener_svd/wiener_all.root\n",
    "\n",
    "#f_merged_asimov = uproot.open(\"numuCC_3d_data/real_data/merge_xs_data.root\") # only using predictions from this file, so it's treated as Asimov\n",
    "#f_wiener_asimov = uproot.open(\"numuCC_3d_data/asimov_data/wiener_asimov.root\")\n",
    "\n",
    "# commenting this out just so we don't accidentally use it\n",
    "#f_merged_data = uproot.open(\"numuCC_3d_data/real_data/merge_xs_data.root\")\n",
    "#f_wiener_data = uproot.open(\"numuCC_3d_data/real_data/wiener_data.root\")\n",
    "\n",
    "f_merged_fake_nuwro = uproot.open(\"numuCC_3d_data/nuwro_fake_data/merge_xs.root\")\n",
    "f_wiener_fake_nuwro = uproot.open(\"numuCC_3d_data/nuwro_fake_data/wiener_all.root\")\n",
    "\n",
    "f_merged_fake_genie_v2 = uproot.open(\"numuCC_3d_data/genie_v2_fake_data/merge_xs.root\")\n",
    "f_wiener_fake_genie_v2 = uproot.open(\"numuCC_3d_data/genie_v2_fake_data/wiener.root\")\n",
    "\n",
    "skip_AxFFCCQEshape_UBGenie = False\n",
    "\n",
    "use_real_data = False\n",
    "use_nuwro_fake_data = False\n",
    "use_genie_v2_fake_data = False\n",
    "\n",
    "#use_real_data = True\n",
    "#use_nuwro_fake_data = True\n",
    "use_genie_v2_fake_data = True\n",
    "\n",
    "collapse_2d = False\n",
    "collapse_1d = True\n",
    "\n",
    "shape_type = \"rate+shape\"\n",
    "#shape_type = \"+100\"\n",
    "#shape_type = \"matrix_breakdown\"\n",
    "\n",
    "regenerate_universes = True\n",
    "\n",
    "assert not use_real_data, \"Not allowed to unblind yet!\"\n",
    "\n",
    "if use_real_data:\n",
    "    f_merged = f_merged_data\n",
    "    f_wiener = f_wiener_data\n",
    "elif use_nuwro_fake_data:\n",
    "    f_merged = f_merged_fake_nuwro\n",
    "    f_wiener = f_wiener_fake_nuwro\n",
    "elif use_genie_v2_fake_data:\n",
    "    f_merged = f_merged_fake_genie_v2\n",
    "    f_wiener = f_wiener_fake_genie_v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt_vars = [\n",
    "    \"nue_score\",\n",
    "    \"numu_score\",\n",
    "    \"numu_cc_flag\"\n",
    "]\n",
    "\n",
    "eval_vars = [\n",
    "    \"run\",\n",
    "    \"subrun\",\n",
    "    \"event\",\n",
    "    \"truth_nuEnergy\",\n",
    "    \"truth_nuPdg\",\n",
    "    \"truth_isCC\",\n",
    "    \"truth_vtxInside\",\n",
    "    \"match_isFC\",\n",
    "    \"match_completeness_energy\",\n",
    "    \"truth_energyInside\",\n",
    "\n",
    "    \"weight_cv\",\n",
    "    \"weight_spline\",\n",
    "]\n",
    "\n",
    "eval_data_vars = [\n",
    "    \"match_isFC\",\n",
    "]\n",
    "\n",
    "kine_vars = [\n",
    "    \"kine_reco_Enu\",\n",
    "]\n",
    "\n",
    "pf_vars = [\n",
    "    \"reco_muonMomentum\",\n",
    "    \"truth_muonMomentum\",\n",
    "]\n",
    "\n",
    "pf_data_vars = [\n",
    "    \"reco_muonMomentum\",\n",
    "]\n",
    "\n",
    "weight_vars = [\n",
    "    # the framework never uses these, it uses the ones in T_eval instead!\n",
    "    #\"weight_cv\",\n",
    "    #\"weight_spline\",\n",
    "\n",
    "    \"All_UBGenie\",\n",
    "    \n",
    "    \"AxFFCCQEshape_UBGenie\",\n",
    "    \"DecayAngMEC_UBGenie\",\n",
    "    \"NormCCCOH_UBGenie\",\n",
    "    \"NormNCCOH_UBGenie\",\n",
    "    \"RPA_CCQE_UBGenie\",\n",
    "    \"ThetaDelta2NRad_UBGenie\",\n",
    "    \"Theta_Delta2Npi_UBGenie\",\n",
    "    \"VecFFCCQEshape_UBGenie\",\n",
    "    \"XSecShape_CCMEC_UBGenie\",\n",
    "    \"xsr_scc_Fa3_SCC\",\n",
    "    \"xsr_scc_Fv3_SCC\",\n",
    "]\n",
    "\n",
    "#loc = \"/Users/leehagaman/data/processed_checkout_rootfiles/\"\n",
    "loc = \"/Users/leehagaman/data/from_london/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = uproot3.open(loc + \"prodgenie_bnb_nu_overlay_run1/UBGenieFluxSmallUni.root\")[\"wcpselection\"]\n",
    "f_bdt = f[\"T_BDTvars\"].pandas.df(bdt_vars, flatten=False)\n",
    "f_eval = f[\"T_eval\"].pandas.df(eval_vars, flatten=False)\n",
    "f_kine = f[\"T_KINEvars\"].pandas.df(kine_vars, flatten=False)\n",
    "f_pfeval = f[\"T_PFeval\"].pandas.df(pf_vars, flatten=False)\n",
    "f_weight = f[\"T_weight\"].pandas.df(weight_vars, flatten=False)\n",
    "nu_overlay_run1_vars_pot = np.sum(f[\"T_pot\"].pandas.df(\"pot_tor875good\", flatten=False)[\"pot_tor875good\"].to_numpy())\n",
    "nu_overlay_run1_df = pd.concat([f_bdt, f_eval, f_kine, f_pfeval, f_weight], axis=1, sort=False)\n",
    "nu_overlay_run1_df[\"file\"] = \"nu_overlay_run1\"\n",
    "del f\n",
    "del f_bdt\n",
    "del f_eval\n",
    "del f_kine\n",
    "del f_pfeval\n",
    "del f_weight\n",
    "\n",
    "f = uproot3.open(loc + \"prodgenie_bnb_nu_overlay_run2/UBGenieFluxSmallUni.root\")[\"wcpselection\"]\n",
    "f_bdt = f[\"T_BDTvars\"].pandas.df(bdt_vars, flatten=False)\n",
    "f_eval = f[\"T_eval\"].pandas.df(eval_vars, flatten=False)\n",
    "f_kine = f[\"T_KINEvars\"].pandas.df(kine_vars, flatten=False)\n",
    "f_pfeval = f[\"T_PFeval\"].pandas.df(pf_vars, flatten=False)\n",
    "f_weight = f[\"T_weight\"].pandas.df(weight_vars, flatten=False)\n",
    "nu_overlay_run2_vars_pot = np.sum(f[\"T_pot\"].pandas.df(\"pot_tor875good\", flatten=False)[\"pot_tor875good\"].to_numpy())\n",
    "nu_overlay_run2_df = pd.concat([f_bdt, f_eval, f_kine, f_pfeval, f_weight], axis=1, sort=False)\n",
    "nu_overlay_run2_df[\"file\"] = \"nu_overlay_run2\"\n",
    "del f\n",
    "del f_bdt\n",
    "del f_eval\n",
    "del f_kine\n",
    "del f_pfeval\n",
    "del f_weight\n",
    "\n",
    "f = uproot3.open(loc + \"prodgenie_bnb_nu_overlay_run3/UBGenieFluxSmallUni.root\")[\"wcpselection\"]\n",
    "f_bdt = f[\"T_BDTvars\"].pandas.df(bdt_vars, flatten=False)\n",
    "f_eval = f[\"T_eval\"].pandas.df(eval_vars, flatten=False)\n",
    "f_kine = f[\"T_KINEvars\"].pandas.df(kine_vars, flatten=False)\n",
    "f_pfeval = f[\"T_PFeval\"].pandas.df(pf_vars, flatten=False)\n",
    "f_weight = f[\"T_weight\"].pandas.df(weight_vars, flatten=False)\n",
    "nu_overlay_run3_vars_pot = np.sum(f[\"T_pot\"].pandas.df(\"pot_tor875good\", flatten=False)[\"pot_tor875good\"].to_numpy())\n",
    "nu_overlay_run3_df = pd.concat([f_bdt, f_eval, f_kine, f_pfeval, f_weight], axis=1, sort=False)\n",
    "nu_overlay_run3_df[\"file\"] = \"nu_overlay_run3\"\n",
    "del f\n",
    "del f_bdt\n",
    "del f_eval\n",
    "del f_kine\n",
    "del f_pfeval\n",
    "del f_weight\n",
    "\n",
    "print(nu_overlay_run1_df.shape)\n",
    "print(nu_overlay_run2_df.shape)\n",
    "print(nu_overlay_run3_df.shape)\n",
    "\n",
    "nu_overlay_vars_df = pd.concat([\n",
    "    nu_overlay_run1_df, \n",
    "    nu_overlay_run2_df, \n",
    "    nu_overlay_run3_df], sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = uproot3.open(loc + \"checkout_prodgenie_bnb_nu_overlay_run1.root\")[\"wcpselection\"]\n",
    "f_bdt = f[\"T_BDTvars\"].pandas.df(bdt_vars, flatten=False)\n",
    "f_eval = f[\"T_eval\"].pandas.df(eval_vars, flatten=False)\n",
    "f_kine = f[\"T_KINEvars\"].pandas.df(kine_vars, flatten=False)\n",
    "f_pfeval = f[\"T_PFeval\"].pandas.df(pf_vars, flatten=False)\n",
    "nu_overlay_run1_pot = np.sum(f[\"T_pot\"].pandas.df(\"pot_tor875good\", flatten=False)[\"pot_tor875good\"].to_numpy())\n",
    "nu_overlay_run1_df = pd.concat([f_bdt, f_eval, f_kine, f_pfeval], axis=1, sort=False)\n",
    "nu_overlay_run1_df[\"file\"] = \"nu_overlay_run1\"\n",
    "del f\n",
    "del f_bdt\n",
    "del f_eval\n",
    "del f_kine\n",
    "del f_pfeval\n",
    "\n",
    "f = uproot3.open(loc + \"checkout_prodgenie_bnb_nu_overlay_run2.root\")[\"wcpselection\"]\n",
    "f_bdt = f[\"T_BDTvars\"].pandas.df(bdt_vars, flatten=False)\n",
    "f_eval = f[\"T_eval\"].pandas.df(eval_vars, flatten=False)\n",
    "f_kine = f[\"T_KINEvars\"].pandas.df(kine_vars, flatten=False)\n",
    "f_pfeval = f[\"T_PFeval\"].pandas.df(pf_vars, flatten=False)\n",
    "nu_overlay_run2_pot = np.sum(f[\"T_pot\"].pandas.df(\"pot_tor875good\", flatten=False)[\"pot_tor875good\"].to_numpy())\n",
    "nu_overlay_run2_df = pd.concat([f_bdt, f_eval, f_kine, f_pfeval], axis=1, sort=False)\n",
    "nu_overlay_run2_df[\"file\"] = \"nu_overlay_run2\"\n",
    "del f\n",
    "del f_bdt\n",
    "del f_eval\n",
    "del f_kine\n",
    "del f_pfeval\n",
    "\n",
    "f = uproot3.open(loc + \"checkout_prodgenie_bnb_nu_overlay_run3.root\")[\"wcpselection\"]\n",
    "f_bdt = f[\"T_BDTvars\"].pandas.df(bdt_vars, flatten=False)\n",
    "f_eval = f[\"T_eval\"].pandas.df(eval_vars, flatten=False)\n",
    "f_kine = f[\"T_KINEvars\"].pandas.df(kine_vars, flatten=False)\n",
    "f_pfeval = f[\"T_PFeval\"].pandas.df(pf_vars, flatten=False)\n",
    "nu_overlay_run3_pot = np.sum(f[\"T_pot\"].pandas.df(\"pot_tor875good\", flatten=False)[\"pot_tor875good\"].to_numpy())\n",
    "nu_overlay_run3_df = pd.concat([f_bdt, f_eval, f_kine, f_pfeval], axis=1, sort=False)\n",
    "nu_overlay_run3_df[\"file\"] = \"nu_overlay_run3\"\n",
    "del f\n",
    "del f_bdt\n",
    "del f_eval\n",
    "del f_kine\n",
    "del f_pfeval\n",
    "\n",
    "print(nu_overlay_run1_df.shape)\n",
    "print(nu_overlay_run2_df.shape)\n",
    "print(nu_overlay_run3_df.shape)\n",
    "\n",
    "nu_overlay_df = pd.concat([\n",
    "    nu_overlay_run1_df, \n",
    "    nu_overlay_run2_df, \n",
    "    nu_overlay_run3_df], sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unisim_variations_dic = {}\n",
    "for unisim_type in [\"AxFFCCQEshape_UBGenie\",\n",
    "                                    \"DecayAngMEC_UBGenie\",\n",
    "                                    \"NormCCCOH_UBGenie\",\n",
    "                                    \"NormNCCOH_UBGenie\",\n",
    "                                    \"RPA_CCQE_UBGenie\",\n",
    "                                    \"ThetaDelta2NRad_UBGenie\",\n",
    "                                    \"Theta_Delta2Npi_UBGenie\",\n",
    "                                    \"VecFFCCQEshape_UBGenie\",\n",
    "                                    \"XSecShape_CCMEC_UBGenie\",\n",
    "                                    \"xsr_scc_Fa3_SCC\",\n",
    "                                    \"xsr_scc_Fv3_SCC\"]:\n",
    "\n",
    "    num_unisim_variations_dic[unisim_type] = len(nu_overlay_vars_df[unisim_type].to_numpy()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = uproot3.open(loc + \"prodgenie_dirt_overlay_run1_all/UBGenieFluxSmallUni.root\")[\"wcpselection\"]\n",
    "f_bdt = f[\"T_BDTvars\"].pandas.df(bdt_vars, flatten=False)\n",
    "f_eval = f[\"T_eval\"].pandas.df(eval_vars, flatten=False)\n",
    "f_kine = f[\"T_KINEvars\"].pandas.df(kine_vars, flatten=False)\n",
    "f_pfeval = f[\"T_PFeval\"].pandas.df(pf_vars, flatten=False)\n",
    "f_weight = f[\"T_weight\"].pandas.df(weight_vars, flatten=False)\n",
    "dirt_run1_vars_pot = np.sum(f[\"T_pot\"].pandas.df(\"pot_tor875good\", flatten=False)[\"pot_tor875good\"].to_numpy())\n",
    "dirt_run1_df = pd.concat([f_bdt, f_eval, f_kine, f_pfeval, f_weight], axis=1, sort=False)\n",
    "dirt_run1_df[\"file\"] = \"dirt_run1\"\n",
    "del f\n",
    "del f_bdt\n",
    "del f_eval\n",
    "del f_kine\n",
    "del f_pfeval\n",
    "del f_weight\n",
    "\n",
    "f = uproot3.open(loc + \"prodgenie_dirt_overlay_run2_all/UBGenieFluxSmallUni.root\")[\"wcpselection\"]\n",
    "f_bdt = f[\"T_BDTvars\"].pandas.df(bdt_vars, flatten=False)\n",
    "f_eval = f[\"T_eval\"].pandas.df(eval_vars, flatten=False)\n",
    "f_kine = f[\"T_KINEvars\"].pandas.df(kine_vars, flatten=False)\n",
    "f_pfeval = f[\"T_PFeval\"].pandas.df(pf_vars, flatten=False)\n",
    "f_weight = f[\"T_weight\"].pandas.df(weight_vars, flatten=False)\n",
    "dirt_run2_vars_pot = np.sum(f[\"T_pot\"].pandas.df(\"pot_tor875good\", flatten=False)[\"pot_tor875good\"].to_numpy())\n",
    "dirt_run2_df = pd.concat([f_bdt, f_eval, f_kine, f_pfeval, f_weight], axis=1, sort=False)\n",
    "dirt_run2_df[\"file\"] = \"dirt_run2\"\n",
    "del f\n",
    "del f_bdt\n",
    "del f_eval\n",
    "del f_kine\n",
    "del f_pfeval\n",
    "del f_weight\n",
    "\n",
    "f = uproot3.open(loc + \"prodgenie_dirt_overlay_run3_all/UBGenieFluxSmallUni.root\")[\"wcpselection\"]\n",
    "f_bdt = f[\"T_BDTvars\"].pandas.df(bdt_vars, flatten=False)\n",
    "f_eval = f[\"T_eval\"].pandas.df(eval_vars, flatten=False)\n",
    "f_kine = f[\"T_KINEvars\"].pandas.df(kine_vars, flatten=False)\n",
    "f_pfeval = f[\"T_PFeval\"].pandas.df(pf_vars, flatten=False)\n",
    "f_weight = f[\"T_weight\"].pandas.df(weight_vars, flatten=False)\n",
    "dirt_run3_vars_pot = np.sum(f[\"T_pot\"].pandas.df(\"pot_tor875good\", flatten=False)[\"pot_tor875good\"].to_numpy())\n",
    "dirt_run3_df = pd.concat([f_bdt, f_eval, f_kine, f_pfeval, f_weight], axis=1, sort=False)\n",
    "dirt_run3_df[\"file\"] = \"dirt_run3\"\n",
    "del f\n",
    "del f_bdt\n",
    "del f_eval\n",
    "del f_kine\n",
    "del f_pfeval\n",
    "del f_weight\n",
    "\n",
    "print(dirt_run1_df.shape)\n",
    "print(dirt_run2_df.shape)\n",
    "print(dirt_run3_df.shape)\n",
    "\n",
    "dirt_vars_df = pd.concat([\n",
    "    dirt_run1_df, \n",
    "    dirt_run2_df, \n",
    "    dirt_run3_df], sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = uproot3.open(loc + \"checkout_prodgenie_dirt_overlay_run1_all.root\")[\"wcpselection\"]\n",
    "f_bdt = f[\"T_BDTvars\"].pandas.df(bdt_vars, flatten=False)\n",
    "f_eval = f[\"T_eval\"].pandas.df(eval_vars, flatten=False)\n",
    "f_kine = f[\"T_KINEvars\"].pandas.df(kine_vars, flatten=False)\n",
    "f_pfeval = f[\"T_PFeval\"].pandas.df(pf_vars, flatten=False)\n",
    "dirt_run1_pot = np.sum(f[\"T_pot\"].pandas.df(\"pot_tor875good\", flatten=False)[\"pot_tor875good\"].to_numpy())\n",
    "dirt_run1_df = pd.concat([f_bdt, f_eval, f_kine, f_pfeval], axis=1, sort=False)\n",
    "dirt_run1_df[\"file\"] = \"dirt_run1\"\n",
    "del f\n",
    "del f_bdt\n",
    "del f_eval\n",
    "del f_kine\n",
    "del f_pfeval\n",
    "\n",
    "f = uproot3.open(loc + \"checkout_prodgenie_dirt_overlay_run2_all.root\")[\"wcpselection\"]\n",
    "f_bdt = f[\"T_BDTvars\"].pandas.df(bdt_vars, flatten=False)\n",
    "f_eval = f[\"T_eval\"].pandas.df(eval_vars, flatten=False)\n",
    "f_kine = f[\"T_KINEvars\"].pandas.df(kine_vars, flatten=False)\n",
    "f_pfeval = f[\"T_PFeval\"].pandas.df(pf_vars, flatten=False)\n",
    "dirt_run2_pot = np.sum(f[\"T_pot\"].pandas.df(\"pot_tor875good\", flatten=False)[\"pot_tor875good\"].to_numpy())\n",
    "dirt_run2_df = pd.concat([f_bdt, f_eval, f_kine, f_pfeval], axis=1, sort=False)\n",
    "dirt_run2_df[\"file\"] = \"dirt_run2\"\n",
    "del f\n",
    "del f_bdt\n",
    "del f_eval\n",
    "del f_kine\n",
    "del f_pfeval\n",
    "\n",
    "f = uproot3.open(loc + \"checkout_prodgenie_dirt_overlay_run3_all.root\")[\"wcpselection\"]\n",
    "f_bdt = f[\"T_BDTvars\"].pandas.df(bdt_vars, flatten=False)\n",
    "f_eval = f[\"T_eval\"].pandas.df(eval_vars, flatten=False)\n",
    "f_kine = f[\"T_KINEvars\"].pandas.df(kine_vars, flatten=False)\n",
    "f_pfeval = f[\"T_PFeval\"].pandas.df(pf_vars, flatten=False)\n",
    "dirt_run3_pot = np.sum(f[\"T_pot\"].pandas.df(\"pot_tor875good\", flatten=False)[\"pot_tor875good\"].to_numpy())\n",
    "dirt_run3_df = pd.concat([f_bdt, f_eval, f_kine, f_pfeval], axis=1, sort=False)\n",
    "dirt_run3_df[\"file\"] = \"dirt_run3\"\n",
    "del f\n",
    "del f_bdt\n",
    "del f_eval\n",
    "del f_kine\n",
    "del f_pfeval\n",
    "\n",
    "print(dirt_run1_df.shape)\n",
    "print(dirt_run2_df.shape)\n",
    "print(dirt_run3_df.shape)\n",
    "\n",
    "dirt_df = pd.concat([\n",
    "    dirt_run1_df, \n",
    "    dirt_run2_df, \n",
    "    dirt_run3_df], sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = uproot3.open(loc + \"wcp_data_extbnb_run1_mcc9_v08_00_00_53_checkout.root\")[\"wcpselection\"]\n",
    "f_bdt = f[\"T_BDTvars\"].pandas.df(bdt_vars, flatten=False)\n",
    "f_eval = f[\"T_eval\"].pandas.df(eval_data_vars, flatten=False)\n",
    "f_kine = f[\"T_KINEvars\"].pandas.df(kine_vars, flatten=False)\n",
    "f_pfeval = f[\"T_PFeval\"].pandas.df(pf_data_vars, flatten=False)\n",
    "ext_run1_df = pd.concat([f_bdt, f_eval, f_kine, f_pfeval], axis=1, sort=False)\n",
    "ext_run1_df[\"file\"] = \"ext_run1\"\n",
    "del f\n",
    "del f_bdt\n",
    "del f_eval\n",
    "del f_kine\n",
    "del f_pfeval\n",
    "\n",
    "f = uproot3.open(loc + \"wcp_data_extbnb_run2_mcc9_v08_00_00_53_checkout.root\")[\"wcpselection\"]\n",
    "f_bdt = f[\"T_BDTvars\"].pandas.df(bdt_vars, flatten=False)\n",
    "f_eval = f[\"T_eval\"].pandas.df(eval_data_vars, flatten=False)\n",
    "f_kine = f[\"T_KINEvars\"].pandas.df(kine_vars, flatten=False)\n",
    "f_pfeval = f[\"T_PFeval\"].pandas.df(pf_data_vars, flatten=False)\n",
    "ext_run2_df = pd.concat([f_bdt, f_eval, f_kine, f_pfeval], axis=1, sort=False)\n",
    "ext_run2_df[\"file\"] = \"ext_run2\"\n",
    "del f\n",
    "del f_bdt\n",
    "del f_eval\n",
    "del f_kine\n",
    "del f_pfeval\n",
    "\n",
    "f = uproot3.open(loc + \"wcp_data_extbnb_run3_mcc9_v08_00_00_53_checkout.root\")[\"wcpselection\"]\n",
    "f_bdt = f[\"T_BDTvars\"].pandas.df(bdt_vars, flatten=False)\n",
    "f_eval = f[\"T_eval\"].pandas.df(eval_data_vars, flatten=False)\n",
    "f_kine = f[\"T_KINEvars\"].pandas.df(kine_vars, flatten=False)\n",
    "f_pfeval = f[\"T_PFeval\"].pandas.df(pf_data_vars, flatten=False)\n",
    "ext_run3_df = pd.concat([f_bdt, f_eval, f_kine, f_pfeval], axis=1, sort=False)\n",
    "ext_run3_df[\"file\"] = \"ext_run3\"\n",
    "del f\n",
    "del f_bdt\n",
    "del f_eval\n",
    "del f_kine\n",
    "del f_pfeval\n",
    "\n",
    "print(ext_run1_df.shape)\n",
    "print(ext_run2_df.shape)\n",
    "print(ext_run3_df.shape)\n",
    "\n",
    "ext_df = pd.concat([\n",
    "    ext_run1_df, \n",
    "    ext_run2_df, \n",
    "    ext_run3_df], sort=False)\n",
    "\n",
    "print(\"concatenated ext files\")\n",
    "\n",
    "ext_df[\"weight_cv\"] = [1. for _ in range(ext_df.shape[0])]\n",
    "ext_df[\"weight_spline\"] = [1. for _ in range(ext_df.shape[0])]\n",
    "print(\"added first ext weight values\")\n",
    "six_hundred_ones_arr = np.array([1. for _1 in range(600)])\n",
    "All_UBGenie_ones_arr = [six_hundred_ones_arr for _2 in range(ext_df.shape[0])]\n",
    "print(\"made ones array\")\n",
    "ext_df[\"All_UBGenie\"] = All_UBGenie_ones_arr\n",
    "\n",
    "print(\"added All_UBGenie ext weight values\")\n",
    "\n",
    "\n",
    "for unisim_type in [\"AxFFCCQEshape_UBGenie\",\n",
    "                                    \"DecayAngMEC_UBGenie\",\n",
    "                                    \"NormCCCOH_UBGenie\",\n",
    "                                    \"NormNCCOH_UBGenie\",\n",
    "                                    \"RPA_CCQE_UBGenie\",\n",
    "                                    \"ThetaDelta2NRad_UBGenie\",\n",
    "                                    \"Theta_Delta2Npi_UBGenie\",\n",
    "                                    \"VecFFCCQEshape_UBGenie\",\n",
    "                                    \"XSecShape_CCMEC_UBGenie\",\n",
    "                                    \"xsr_scc_Fa3_SCC\",\n",
    "                                    \"xsr_scc_Fv3_SCC\",]:\n",
    "    \n",
    "    print(\"adding ext weight values for unisim:\", unisim_type)\n",
    "\n",
    "    ext_df[unisim_type] = [np.array([1. for _1 in range(num_unisim_variations_dic[unisim_type])]) for _2 in range(ext_df.shape[0])] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([nu_overlay_df, dirt_df, ext_df], sort=False)\n",
    "all_vars_df = pd.concat([nu_overlay_vars_df, dirt_vars_df], sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costheta_vals = []\n",
    "muonmomentum_vals = []\n",
    "reco_muonmomentum_x = all_df[\"reco_muonMomentum[0]\"].to_numpy()\n",
    "reco_muonmomentum_y = all_df[\"reco_muonMomentum[1]\"].to_numpy()\n",
    "reco_muonmomentum_z = all_df[\"reco_muonMomentum[2]\"].to_numpy()\n",
    "reco_muonmomentum_t = all_df[\"reco_muonMomentum[3]\"].to_numpy()\n",
    "for i in range(len(reco_muonmomentum_x)):\n",
    "    if reco_muonmomentum_t[i] < 105.66 / 1000.: # surprising that this happens for positive values, but I did find some events\n",
    "        costheta_vals.append(-1)\n",
    "        muonmomentum_vals.append(-1)\n",
    "    else:\n",
    "        costheta_vals.append(reco_muonmomentum_z[i] / np.sqrt(reco_muonmomentum_x[i]**2 + reco_muonmomentum_y[i]**2 + reco_muonmomentum_z[i]**2))\n",
    "        muon_KE = reco_muonmomentum_t[i] * 1000. - 105.66\n",
    "        muonmomentum_vals.append(np.sqrt(muon_KE**2 + 2 * muon_KE * 105.66))\n",
    "\n",
    "all_df[\"reco_costheta\"] = costheta_vals\n",
    "all_df[\"reco_muon_momentum\"] = muonmomentum_vals\n",
    "\n",
    "\n",
    "costheta_vals = []\n",
    "muonmomentum_vals = []\n",
    "reco_muonmomentum_x = all_vars_df[\"reco_muonMomentum[0]\"].to_numpy()\n",
    "reco_muonmomentum_y = all_vars_df[\"reco_muonMomentum[1]\"].to_numpy()\n",
    "reco_muonmomentum_z = all_vars_df[\"reco_muonMomentum[2]\"].to_numpy()\n",
    "reco_muonmomentum_t = all_vars_df[\"reco_muonMomentum[3]\"].to_numpy()\n",
    "for i in range(len(reco_muonmomentum_x)):\n",
    "    if reco_muonmomentum_t[i] < 105.66 / 1000.: # surprising that this happens for positive values, but I did find some events\n",
    "        costheta_vals.append(-1)\n",
    "        muonmomentum_vals.append(-1)\n",
    "    else:\n",
    "        costheta_vals.append(reco_muonmomentum_z[i] / np.sqrt(reco_muonmomentum_x[i]**2 + reco_muonmomentum_y[i]**2 + reco_muonmomentum_z[i]**2))\n",
    "        muon_KE = reco_muonmomentum_t[i] * 1000. - 105.66\n",
    "        muonmomentum_vals.append(np.sqrt(muon_KE**2 + 2 * muon_KE * 105.66))\n",
    "\n",
    "all_vars_df[\"reco_costheta\"] = costheta_vals\n",
    "all_vars_df[\"reco_muon_momentum\"] = muonmomentum_vals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_real_data:\n",
    "    data_pots = [\n",
    "        1.42319e+20,\n",
    "        2.5413e+20,\n",
    "        2.40466e+20\n",
    "    ]\n",
    "elif use_nuwro_fake_data:\n",
    "    data_pots = [\n",
    "        0,\n",
    "        2.98217e+20,\n",
    "        3.12922e+20\n",
    "    ]\n",
    "elif use_genie_v2_fake_data:\n",
    "    data_pots = [\n",
    "        7.2432440e20, \n",
    "        0., \n",
    "        0.\n",
    "    ]\n",
    "\n",
    "ext_pots = [\n",
    "    2.21814e+20,\n",
    "    6.25014e+20,\n",
    "    7.4127e+20,\n",
    "]\n",
    "\n",
    "if use_real_data:\n",
    "    include_ext = True\n",
    "    include_dirt = True\n",
    "else:\n",
    "    include_ext = False\n",
    "    include_dirt = False\n",
    "\n",
    "\n",
    "weight_cv_vals = all_df[\"weight_cv\"].to_numpy()\n",
    "weight_spline_vals = all_df[\"weight_spline\"].to_numpy()\n",
    "files = all_df[\"file\"].to_numpy()\n",
    "net_weight_vals = []\n",
    "for i in range(len(weight_cv_vals)):\n",
    "    w_cv = weight_cv_vals[i]\n",
    "    if not (0 < w_cv < 30):\n",
    "        w_cv = 1\n",
    "    \n",
    "    if files[i] == \"nu_overlay_run1\":\n",
    "        net_weight_vals.append(w_cv * weight_spline_vals[i] * data_pots[0] / nu_overlay_run1_pot)\n",
    "    elif files[i] == \"nu_overlay_run2\":\n",
    "        net_weight_vals.append(w_cv * weight_spline_vals[i] * data_pots[1] / nu_overlay_run2_pot)\n",
    "    elif files[i] == \"nu_overlay_run3\":\n",
    "        net_weight_vals.append(w_cv * weight_spline_vals[i] * data_pots[2] / nu_overlay_run3_pot)\n",
    "\n",
    "    elif files[i] == \"dirt_run1\":\n",
    "        if include_dirt:\n",
    "            net_weight_vals.append(w_cv * weight_spline_vals[i] * data_pots[0] / dirt_run1_pot)\n",
    "        else:\n",
    "            net_weight_vals.append(0)\n",
    "    elif files[i] == \"dirt_run2\":\n",
    "        if include_dirt:\n",
    "            net_weight_vals.append(w_cv * weight_spline_vals[i] * data_pots[1] / dirt_run2_pot)\n",
    "        else:\n",
    "            net_weight_vals.append(0)\n",
    "    elif files[i] == \"dirt_run3\":\n",
    "        if include_dirt:\n",
    "            net_weight_vals.append(w_cv * weight_spline_vals[i] * data_pots[2] / dirt_run3_pot)\n",
    "        else:\n",
    "            net_weight_vals.append(0)\n",
    "\n",
    "    if files[i] == \"ext_run1\":\n",
    "        if include_ext:\n",
    "            net_weight_vals.append(w_cv * weight_spline_vals[i] * data_pots[0] / ext_pots[0])\n",
    "        else:\n",
    "            net_weight_vals.append(0)\n",
    "    elif files[i] == \"ext_run2\":\n",
    "        if include_ext:\n",
    "            net_weight_vals.append(w_cv * weight_spline_vals[i] * data_pots[1] / ext_pots[1])\n",
    "        else:\n",
    "            net_weight_vals.append(0)\n",
    "    elif files[i] == \"ext_run3\":\n",
    "        if include_ext:\n",
    "            net_weight_vals.append(w_cv * weight_spline_vals[i] * data_pots[2] / ext_pots[2])\n",
    "        else:\n",
    "            net_weight_vals.append(0)\n",
    "    \n",
    "all_df[\"net_weight\"] = net_weight_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_cv_vals = all_vars_df[\"weight_cv\"].to_numpy()\n",
    "weight_spline_vals = all_vars_df[\"weight_spline\"].to_numpy()\n",
    "files = all_vars_df[\"file\"].to_numpy()\n",
    "net_weight_vals = []\n",
    "for i in range(len(weight_cv_vals)):\n",
    "    w_cv = weight_cv_vals[i]\n",
    "    if not (0 < w_cv < 30):\n",
    "        w_cv = 1\n",
    "    if files[i] == \"nu_overlay_run1\":\n",
    "        net_weight_vals.append(w_cv * weight_spline_vals[i] * data_pots[0] / nu_overlay_run1_pot)\n",
    "    elif files[i] == \"nu_overlay_run2\":\n",
    "        net_weight_vals.append(w_cv * weight_spline_vals[i] * data_pots[1] / nu_overlay_run2_pot)\n",
    "    elif files[i] == \"nu_overlay_run3\":\n",
    "        net_weight_vals.append(w_cv * weight_spline_vals[i] * data_pots[2] / nu_overlay_run3_pot)\n",
    "    elif files[i] == \"dirt_run1\":\n",
    "        if include_dirt:\n",
    "            net_weight_vals.append(w_cv * weight_spline_vals[i] * data_pots[0] / dirt_run1_pot)\n",
    "        else:\n",
    "            net_weight_vals.append(0)\n",
    "    elif files[i] == \"dirt_run2\":\n",
    "        if include_dirt:\n",
    "            net_weight_vals.append(w_cv * weight_spline_vals[i] * data_pots[1] / dirt_run2_pot)\n",
    "        else:\n",
    "            net_weight_vals.append(0)\n",
    "    elif files[i] == \"dirt_run3\":\n",
    "        if include_dirt:\n",
    "            net_weight_vals.append(w_cv * weight_spline_vals[i] * data_pots[2] / dirt_run3_pot)\n",
    "        else:\n",
    "            net_weight_vals.append(0)\n",
    "    \n",
    "all_vars_df[\"net_weight\"] = net_weight_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[\"truth_muonMomentum_3\"] = all_df[\"truth_muonMomentum[3]\"].to_numpy()\n",
    "all_df[\"true_muon_KE\"] = all_df[\"truth_muonMomentum_3\"].to_numpy()*1000.-105.66\n",
    "all_df[\"true_muon_momentum\"] = np.sqrt(all_df[\"true_muon_KE\"]**2 + 2*all_df[\"true_muon_KE\"]*105.66)\n",
    "\n",
    "all_vars_df[\"truth_muonMomentum_3\"] = all_vars_df[\"truth_muonMomentum[3]\"].to_numpy()\n",
    "all_vars_df[\"true_muon_KE\"] = all_vars_df[\"truth_muonMomentum_3\"].to_numpy()*1000.-105.66\n",
    "all_vars_df[\"true_muon_momentum\"] = np.sqrt(all_vars_df[\"true_muon_KE\"]**2 + 2*all_vars_df[\"true_muon_KE\"]*105.66)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df = all_df.query(\"numu_cc_flag >= 0 and numu_score > 0.9 and nue_score < 7 and reco_muon_momentum>0\")\n",
    "selected_vars_df = all_vars_df.query(\"numu_cc_flag >= 0 and numu_score > 0.9 and nue_score < 7 and reco_muon_momentum>0\")\n",
    "\n",
    "sig_query = \"match_completeness_energy>0.1*truth_energyInside and truth_nuPdg==14 and truth_isCC==1 and truth_vtxInside==1 and truth_muonMomentum_3>0 and truth_nuEnergy<=4000 and truth_nuEnergy > 200 and true_muon_momentum > 0 and true_muon_momentum <= 2500\"\n",
    "\n",
    "sig_sel_df = selected_df.query(sig_query)\n",
    "bkg_sel_df = selected_df.query(f\"not ({sig_query})\")\n",
    "mc_bkg_sel_df = bkg_sel_df.query(\"file != 'ext_run1' and file != 'ext_run2' and file != 'ext_run3'\")\n",
    "ext_sel_df = bkg_sel_df.query(\"file == 'ext_run1' or file == 'ext_run2' or file == 'ext_run3'\")\n",
    "\n",
    "print(sig_sel_df.shape)\n",
    "print(bkg_sel_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_sel_df[[\"file\", \"net_weight\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(sig_sel_df[\"net_weight\"].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"plt.figure()\n",
    "plt.hist(all_df[\"file\"].to_numpy(), bins=100)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(selected_df[\"file\"].to_numpy(), bins=100)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_hist = []\n",
    "\n",
    "for containment in [\"FC\", \"PC\"]:\n",
    "    \n",
    "    if containment == \"FC\":\n",
    "        containment_df = selected_df.query(\"match_isFC==1\")\n",
    "    else:\n",
    "        containment_df = selected_df.query(\"match_isFC==0\")\n",
    "        \n",
    "    for Enu_bin in range(4):\n",
    "        \n",
    "        if Enu_bin == 0:\n",
    "            Enu_df = containment_df.query(\"200 < kine_reco_Enu <= 705\")\n",
    "        elif Enu_bin == 1:\n",
    "            Enu_df = containment_df.query(\"705 < kine_reco_Enu < 1050\")\n",
    "        elif Enu_bin == 2:\n",
    "            Enu_df = containment_df.query(\"1050 < kine_reco_Enu < 1570\")\n",
    "        elif Enu_bin == 3:\n",
    "            Enu_df = containment_df.query(\"1570 < kine_reco_Enu < 4000\")\n",
    "        \n",
    "        for theta_bin in range(9):\n",
    "            \n",
    "            if theta_bin == 0:\n",
    "                theta_df = Enu_df.query(\"-1 < reco_costheta <= -0.5\")\n",
    "            elif theta_bin == 1:\n",
    "                theta_df = Enu_df.query(\"-0.5 < reco_costheta <= 0.\")\n",
    "            elif theta_bin == 2:\n",
    "                theta_df = Enu_df.query(\"0. < reco_costheta <= 0.27\")\n",
    "            elif theta_bin == 3:\n",
    "                theta_df = Enu_df.query(\"0.27 < reco_costheta <= 0.45\")\n",
    "            elif theta_bin == 4:\n",
    "                theta_df = Enu_df.query(\"0.45 < reco_costheta <= 0.62\")\n",
    "            elif theta_bin == 5:\n",
    "                theta_df = Enu_df.query(\"0.62 < reco_costheta <= 0.76\")\n",
    "            elif theta_bin == 6:\n",
    "                theta_df = Enu_df.query(\"0.76 < reco_costheta <= 0.86\")\n",
    "            elif theta_bin == 7:\n",
    "                theta_df = Enu_df.query(\"0.86 < reco_costheta <= 0.94\")\n",
    "            else:\n",
    "                theta_df = Enu_df.query(\"0.94 < reco_costheta <= 1.\")\n",
    "            \n",
    "            reco_hist += list(np.histogram(theta_df[\"reco_muon_momentum\"].to_numpy(), \n",
    "                                          weights=theta_df[\"net_weight\"].to_numpy(),\n",
    "                                          bins = [i*100 for i in range(16)] + [1e9] # fifteen bins from 0 to 1500 plus an overflow\n",
    "                                         )[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_root_pred = []\n",
    "\n",
    "for containment in [\"FC\", \"PC\"]:\n",
    "    \n",
    "    if containment == \"FC\":\n",
    "        containment_df = sig_sel_df.query(\"match_isFC==1\")\n",
    "    else:\n",
    "        containment_df = sig_sel_df.query(\"match_isFC==0\")\n",
    "        \n",
    "    for Enu_bin in range(4):\n",
    "        \n",
    "        if Enu_bin == 0:\n",
    "            Enu_df = containment_df.query(\"200 < kine_reco_Enu <= 705\")\n",
    "        elif Enu_bin == 1:\n",
    "            Enu_df = containment_df.query(\"705 < kine_reco_Enu < 1050\")\n",
    "        elif Enu_bin == 2:\n",
    "            Enu_df = containment_df.query(\"1050 < kine_reco_Enu < 1570\")\n",
    "        elif Enu_bin == 3:\n",
    "            Enu_df = containment_df.query(\"1570 < kine_reco_Enu < 4000\")\n",
    "        \n",
    "        for theta_bin in range(9):\n",
    "            \n",
    "            if theta_bin == 0:\n",
    "                theta_df = Enu_df.query(\"-1 < reco_costheta <= -0.5\")\n",
    "            elif theta_bin == 1:\n",
    "                theta_df = Enu_df.query(\"-0.5 < reco_costheta <= 0.\")\n",
    "            elif theta_bin == 2:\n",
    "                theta_df = Enu_df.query(\"0. < reco_costheta <= 0.27\")\n",
    "            elif theta_bin == 3:\n",
    "                theta_df = Enu_df.query(\"0.27 < reco_costheta <= 0.45\")\n",
    "            elif theta_bin == 4:\n",
    "                theta_df = Enu_df.query(\"0.45 < reco_costheta <= 0.62\")\n",
    "            elif theta_bin == 5:\n",
    "                theta_df = Enu_df.query(\"0.62 < reco_costheta <= 0.76\")\n",
    "            elif theta_bin == 6:\n",
    "                theta_df = Enu_df.query(\"0.76 < reco_costheta <= 0.86\")\n",
    "            elif theta_bin == 7:\n",
    "                theta_df = Enu_df.query(\"0.86 < reco_costheta <= 0.94\")\n",
    "            else:\n",
    "                theta_df = Enu_df.query(\"0.94 < reco_costheta <= 1.\")\n",
    "            \n",
    "            sig_root_pred += list(np.histogram(theta_df[\"reco_muon_momentum\"].to_numpy(), \n",
    "                                          weights=theta_df[\"net_weight\"].to_numpy(),\n",
    "                                          bins = [i*100 for i in range(16)] + [1e9] # fifteen bins from 0 to 1500 plus an overflow\n",
    "                                         )[0])\n",
    "            \n",
    "print(len(sig_root_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_bkg_root_pred = []\n",
    "\n",
    "for containment in [\"FC\", \"PC\"]:\n",
    "    \n",
    "    if containment == \"FC\":\n",
    "        containment_df = mc_bkg_sel_df.query(\"match_isFC==1\")\n",
    "    else:\n",
    "        containment_df = mc_bkg_sel_df.query(\"match_isFC==0\")\n",
    "        \n",
    "    for Enu_bin in range(4):\n",
    "        \n",
    "        if Enu_bin == 0:\n",
    "            Enu_df = containment_df.query(\"200 < kine_reco_Enu <= 705\")\n",
    "        elif Enu_bin == 1:\n",
    "            Enu_df = containment_df.query(\"705 < kine_reco_Enu < 1050\")\n",
    "        elif Enu_bin == 2:\n",
    "            Enu_df = containment_df.query(\"1050 < kine_reco_Enu < 1570\")\n",
    "        elif Enu_bin == 3:\n",
    "            Enu_df = containment_df.query(\"1570 < kine_reco_Enu < 4000\")\n",
    "        \n",
    "        for theta_bin in range(9):\n",
    "            \n",
    "            if theta_bin == 0:\n",
    "                theta_df = Enu_df.query(\"-1 < reco_costheta <= -0.5\")\n",
    "            elif theta_bin == 1:\n",
    "                theta_df = Enu_df.query(\"-0.5 < reco_costheta <= 0.\")\n",
    "            elif theta_bin == 2:\n",
    "                theta_df = Enu_df.query(\"0. < reco_costheta <= 0.27\")\n",
    "            elif theta_bin == 3:\n",
    "                theta_df = Enu_df.query(\"0.27 < reco_costheta <= 0.45\")\n",
    "            elif theta_bin == 4:\n",
    "                theta_df = Enu_df.query(\"0.45 < reco_costheta <= 0.62\")\n",
    "            elif theta_bin == 5:\n",
    "                theta_df = Enu_df.query(\"0.62 < reco_costheta <= 0.76\")\n",
    "            elif theta_bin == 6:\n",
    "                theta_df = Enu_df.query(\"0.76 < reco_costheta <= 0.86\")\n",
    "            elif theta_bin == 7:\n",
    "                theta_df = Enu_df.query(\"0.86 < reco_costheta <= 0.94\")\n",
    "            else:\n",
    "                theta_df = Enu_df.query(\"0.94 < reco_costheta <= 1.\")\n",
    "            \n",
    "            mc_bkg_root_pred += list(np.histogram(theta_df[\"reco_muon_momentum\"].to_numpy(), \n",
    "                                          weights=theta_df[\"net_weight\"].to_numpy(),\n",
    "                                          bins = [i*100 for i in range(16)] + [1e9] # fifteen bins from 0 to 1500 plus an overflow\n",
    "                                         )[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_root_pred = []\n",
    "\n",
    "for containment in [\"FC\", \"PC\"]:\n",
    "    \n",
    "    if containment == \"FC\":\n",
    "        containment_df = ext_sel_df.query(\"match_isFC==1\")\n",
    "    else:\n",
    "        containment_df = ext_sel_df.query(\"match_isFC==0\")\n",
    "        \n",
    "    for Enu_bin in range(4):\n",
    "        \n",
    "        if Enu_bin == 0:\n",
    "            Enu_df = containment_df.query(\"200 < kine_reco_Enu <= 705\")\n",
    "        elif Enu_bin == 1:\n",
    "            Enu_df = containment_df.query(\"705 < kine_reco_Enu < 1050\")\n",
    "        elif Enu_bin == 2:\n",
    "            Enu_df = containment_df.query(\"1050 < kine_reco_Enu < 1570\")\n",
    "        elif Enu_bin == 3:\n",
    "            Enu_df = containment_df.query(\"1570 < kine_reco_Enu < 4000\")\n",
    "        \n",
    "        for theta_bin in range(9):\n",
    "            \n",
    "            if theta_bin == 0:\n",
    "                theta_df = Enu_df.query(\"-1 < reco_costheta <= -0.5\")\n",
    "            elif theta_bin == 1:\n",
    "                theta_df = Enu_df.query(\"-0.5 < reco_costheta <= 0.\")\n",
    "            elif theta_bin == 2:\n",
    "                theta_df = Enu_df.query(\"0. < reco_costheta <= 0.27\")\n",
    "            elif theta_bin == 3:\n",
    "                theta_df = Enu_df.query(\"0.27 < reco_costheta <= 0.45\")\n",
    "            elif theta_bin == 4:\n",
    "                theta_df = Enu_df.query(\"0.45 < reco_costheta <= 0.62\")\n",
    "            elif theta_bin == 5:\n",
    "                theta_df = Enu_df.query(\"0.62 < reco_costheta <= 0.76\")\n",
    "            elif theta_bin == 6:\n",
    "                theta_df = Enu_df.query(\"0.76 < reco_costheta <= 0.86\")\n",
    "            elif theta_bin == 7:\n",
    "                theta_df = Enu_df.query(\"0.86 < reco_costheta <= 0.94\")\n",
    "            else:\n",
    "                theta_df = Enu_df.query(\"0.94 < reco_costheta <= 1.\")\n",
    "            \n",
    "            ext_root_pred += list(np.histogram(theta_df[\"reco_muon_momentum\"].to_numpy(), \n",
    "                                          weights=theta_df[\"net_weight\"].to_numpy(),\n",
    "                                          bins = [i*100 for i in range(16)] + [1e9] # fifteen bins from 0 to 1500 plus an overflow\n",
    "                                         )[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_sig_pred = []\n",
    "mc_bkg_pred = []\n",
    "ext_pred = []\n",
    "tot_pred = []\n",
    "data = []\n",
    "tot_pred_from_hmc = []\n",
    "\n",
    "if use_genie_v2_fake_data: # seems like the genie v2 root files don't contain EXT blocks, should be fine since fake data never includes EXT\n",
    "\n",
    "    for i in range(72):\n",
    "        mc_sig_pred += list(f_merged[f\"histo_{i+1}\"].values(flow=True)[1:])\n",
    "        mc_bkg_pred += list(f_merged[f\"histo_{i+1 + 72}\"].values(flow=True)[1:])\n",
    "        ext_pred += [0 for _ in list(f_merged[f\"histo_{i+1 + 1}\"].values(flow=True)[1:])]\n",
    "            \n",
    "        tot_pred += list(f_merged[f\"histo_{i+1}\"].values(flow=True)[1:] + f_merged[f\"histo_{i+1 + 72}\"].values(flow=True)[1:])\n",
    "        \n",
    "        tot_pred_from_hmc += list(f_merged[f\"hmc_obsch_{i+1}\"].values(flow=True)[1:])\n",
    "        \n",
    "        data += list(f_merged[f\"hdata_obsch_{i+1}\"].values(flow=True)[1:])\n",
    "\n",
    "else:\n",
    "\n",
    "    for i in range(72):\n",
    "        mc_sig_pred += list(f_merged[f\"histo_{i+1}\"].values(flow=True)[1:])\n",
    "        mc_bkg_pred += list(f_merged[f\"histo_{i+1 + 72}\"].values(flow=True)[1:])\n",
    "        ext_pred += list(f_merged[f\"histo_{i+1 + 2 * 72}\"].values(flow=True)[1:])\n",
    "            \n",
    "        tot_pred += list(f_merged[f\"histo_{i+1}\"].values(flow=True)[1:] + f_merged[f\"histo_{i+1 + 72}\"].values(flow=True)[1:] + f_merged[f\"histo_{i+1 + 2 * 72}\"].values(flow=True)[1:])\n",
    "        \n",
    "        tot_pred_from_hmc += list(f_merged[f\"hmc_obsch_{i+1}\"].values(flow=True)[1:])\n",
    "        \n",
    "        data += list(f_merged[f\"hdata_obsch_{i+1}\"].values(flow=True)[1:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sig_root_pred, label=\"sig_root_pred, from WC ntuples\")\n",
    "plt.plot(mc_sig_pred, label=\"mc_sig_pred, from merge.root\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.array(sig_root_pred) - np.array(mc_sig_pred), label=\"sig_root_pred - mc_sig_pred\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mc_bkg_root_pred, label=\"bkg_root_pred, from WC ntuples\")\n",
    "plt.plot(mc_bkg_pred, label=\"mc_bkg_pred, from merge.root\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.array(mc_bkg_root_pred) - np.array(mc_bkg_pred), label=\"mc_bkg_root_pred - mc_bkg_pred\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ext_root_pred, label=\"ext_root_pred, from WC ntuples\")\n",
    "plt.plot(ext_pred, label=\"ext_pred, from merge.root\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.array(ext_root_pred) - np.array(ext_pred), label=\"ext_root_pred - ext_pred\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(reco_hist, label=\"reco_hist, from WC ntuples\")\n",
    "plt.plot(tot_pred, label=\"tot_pred, from merge.root\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.array(reco_hist) - np.array(tot_pred), label=\"reco_hist - tot_pred (merge.root - WC ntuples)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_17_from_london = uproot.open(\"numuCC_3d_data/nuwro_fake_data/XsFlux/cov_17.root\")\n",
    "\n",
    "num_reco_bins = 1152\n",
    "num_truth_bins = 3456\n",
    "\n",
    "if use_genie_v2_fake_data: # I think EXT was not included in merge_xs.root, so here we need to add more zeros ourselves\n",
    "    mat_collapse = np.zeros((num_truth_bins, num_reco_bins))\n",
    "    mat_collapse[:2304, :] = f_merged[\"mat_collapse\"].member(\"fElements\").reshape((2304, num_reco_bins))\n",
    "else:\n",
    "    mat_collapse = f_merged[\"mat_collapse\"].member(\"fElements\").reshape((num_truth_bins, num_reco_bins))\n",
    "\n",
    "if use_genie_v2_fake_data:\n",
    "    cov_17_vec_mean_from_london = mat_collapse.T @ cov_17_from_london[\"vec_mean_17\"].member(\"fElements\")[:num_truth_bins] # EXT is included in vec_mean_17, even though it's not in the covariance matrix?\n",
    "    cov_17_arr_from_london = mat_collapse.T @ cov_17_from_london[\"cov_xf_mat_17\"].member(\"fElements\").reshape((num_truth_bins, num_truth_bins)) @ mat_collapse\n",
    "else:\n",
    "    cov_17_vec_mean_from_london = mat_collapse.T @ cov_17_from_london[\"vec_mean_17\"].member(\"fElements\")\n",
    "    cov_17_arr_from_london = mat_collapse.T @ cov_17_from_london[\"cov_xf_mat_17\"].member(\"fElements\").reshape((num_truth_bins, num_truth_bins)) @ mat_collapse\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(reco_hist, label=\"reco_hist, from WC ntuples\")\n",
    "plt.plot(tot_pred, label=\"tot_pred, from merge.root\")\n",
    "plt.plot(cov_17_vec_mean_from_london, label=\"cov_17_vec_mean_from_london\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.array(cov_17_vec_mean_from_london) - np.array(tot_pred), label=\"cov_17_vec_mean_from_london - tot_pred\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot((np.array(cov_17_vec_mean_from_london) - np.array(tot_pred)) / np.array(tot_pred), label=\"(cov_17_vec_mean_from_london - tot_pred) / tot_pred\")\n",
    "plt.legend()\n",
    "plt.ylim(-0.01, 0.5)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cov_17_vec_mean_from_london[:30], label=\"cov_17_vec_mean_from_london\")\n",
    "plt.plot(tot_pred[:30], label=\"tot_pred\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(cov_17_vec_mean_from_london[:30])\n",
    "print(tot_pred[:30])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot((np.array(cov_17_vec_mean_from_london) - np.array(tot_pred)) / np.array(tot_pred), label=\"(cov_17_vec_mean_from_london - tot_pred) / tot_pred\")\n",
    "plt.legend()\n",
    "plt.ylim(-0.01, 0.1)\n",
    "plt.xlim(0, 30)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(11943936)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if regenerate_universes:\n",
    "\n",
    "    xs_cv_reco_hist = []\n",
    "    universe_reco_hists = [[] for _ in range(600)]\n",
    "\n",
    "    unisim_reco_hist_dic = {}\n",
    "    for unisim_type in [\"AxFFCCQEshape_UBGenie\",\n",
    "                                    \"DecayAngMEC_UBGenie\",\n",
    "                                    \"NormCCCOH_UBGenie\",\n",
    "                                    \"NormNCCOH_UBGenie\",\n",
    "                                    \"RPA_CCQE_UBGenie\",\n",
    "                                    \"ThetaDelta2NRad_UBGenie\",\n",
    "                                    \"Theta_Delta2Npi_UBGenie\",\n",
    "                                    \"VecFFCCQEshape_UBGenie\",\n",
    "                                    \"XSecShape_CCMEC_UBGenie\",\n",
    "                                    \"xsr_scc_Fa3_SCC\",\n",
    "                                    \"xsr_scc_Fv3_SCC\"]:\n",
    "\n",
    "        unisim_reco_hist_dic[unisim_type] = [[] for _ in range(num_unisim_variations_dic[unisim_type])]\n",
    "\n",
    "    muon_momentum_bins = [i*100 for i in range(16)] + [1e9] # fifteen bins from 0 to 1500 plus an overflow\n",
    "\n",
    "    pbar = tqdm(total=2*4*9)\n",
    "\n",
    "    for containment in [\"FC\", \"PC\"]:\n",
    "        if containment == \"FC\":\n",
    "            containment_df = selected_vars_df.query(\"match_isFC==1\")\n",
    "        else:\n",
    "            containment_df = selected_vars_df.query(\"match_isFC==0\")\n",
    "        for Enu_bin in range(4):\n",
    "            if Enu_bin == 0:\n",
    "                Enu_df = containment_df.query(\"200 < kine_reco_Enu <= 705\")\n",
    "            elif Enu_bin == 1:\n",
    "                Enu_df = containment_df.query(\"705 < kine_reco_Enu < 1050\")\n",
    "            elif Enu_bin == 2:\n",
    "                Enu_df = containment_df.query(\"1050 < kine_reco_Enu < 1570\")\n",
    "            elif Enu_bin == 3:\n",
    "                Enu_df = containment_df.query(\"1570 < kine_reco_Enu < 4000\")\n",
    "            for theta_bin in range(9):\n",
    "                if theta_bin == 0:\n",
    "                    theta_df = Enu_df.query(\"-1 < reco_costheta <= -0.5\")\n",
    "                elif theta_bin == 1:\n",
    "                    theta_df = Enu_df.query(\"-0.5 < reco_costheta <= 0.\")\n",
    "                elif theta_bin == 2:\n",
    "                    theta_df = Enu_df.query(\"0. < reco_costheta <= 0.27\")\n",
    "                elif theta_bin == 3:\n",
    "                    theta_df = Enu_df.query(\"0.27 < reco_costheta <= 0.45\")\n",
    "                elif theta_bin == 4:\n",
    "                    theta_df = Enu_df.query(\"0.45 < reco_costheta <= 0.62\")\n",
    "                elif theta_bin == 5:\n",
    "                    theta_df = Enu_df.query(\"0.62 < reco_costheta <= 0.76\")\n",
    "                elif theta_bin == 6:\n",
    "                    theta_df = Enu_df.query(\"0.76 < reco_costheta <= 0.86\")\n",
    "                elif theta_bin == 7:\n",
    "                    theta_df = Enu_df.query(\"0.86 < reco_costheta <= 0.94\")\n",
    "                else:\n",
    "                    theta_df = Enu_df.query(\"0.94 < reco_costheta <= 1.\")\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "                curr_slice_cv = np.histogram(theta_df[\"reco_muon_momentum\"].to_numpy(), weights=theta_df[\"net_weight\"].to_numpy(), bins=muon_momentum_bins)[0]\n",
    "                xs_cv_reco_hist += list(curr_slice_cv)\n",
    "\n",
    "                for i in range(600):\n",
    "                    curr_All_UBGenie_weights = [_[i] for _ in theta_df[\"All_UBGenie\"].to_numpy()]\n",
    "                    rel_weight_diffs = curr_All_UBGenie_weights / theta_df[\"weight_cv\"].to_numpy()\n",
    "                    # https://github.com/BNLIF/wcp-uboone-bdt/blob/main/src/mcm_2.h#L262-L264\n",
    "                    rel_weight_diffs = np.where(np.abs(rel_weight_diffs) > 100, 1, rel_weight_diffs)\n",
    "                    rel_weight_diffs = np.nan_to_num(rel_weight_diffs, nan=0)\n",
    "                    curr_slice_uni = np.histogram(theta_df[\"reco_muon_momentum\"].to_numpy(), weights=theta_df[\"net_weight\"].to_numpy()*rel_weight_diffs, bins=muon_momentum_bins)[0]\n",
    "                    universe_reco_hists[i] += list(curr_slice_uni)\n",
    "\n",
    "                for unisim_type in [\"AxFFCCQEshape_UBGenie\",\n",
    "                                    \"DecayAngMEC_UBGenie\",\n",
    "                                    \"NormCCCOH_UBGenie\",\n",
    "                                    \"NormNCCOH_UBGenie\",\n",
    "                                    \"RPA_CCQE_UBGenie\",\n",
    "                                    \"ThetaDelta2NRad_UBGenie\",\n",
    "                                    \"Theta_Delta2Npi_UBGenie\",\n",
    "                                    \"VecFFCCQEshape_UBGenie\",\n",
    "                                    \"XSecShape_CCMEC_UBGenie\",\n",
    "                                    \"xsr_scc_Fa3_SCC\",\n",
    "                                    \"xsr_scc_Fv3_SCC\",]:\n",
    "\n",
    "                    num_unisim_variations = num_unisim_variations_dic[unisim_type]\n",
    "                    for j in range(num_unisim_variations):    \n",
    "\n",
    "                        curr_weights = [_[j] for _ in theta_df[unisim_type].to_numpy()]\n",
    "                        if not(unisim_type == \"xsr_scc_Fa3_SCC\" or unisim_type == \"xsr_scc_Fv3_SCC\"):\n",
    "                            rel_weight_diffs = curr_weights / theta_df[\"weight_cv\"].to_numpy()\n",
    "\n",
    "                        # https://github.com/BNLIF/wcp-uboone-bdt/blob/main/src/mcm_2.h#L262-L264\n",
    "                        rel_weight_diffs = np.where(np.abs(rel_weight_diffs) > 100, 1, rel_weight_diffs)\n",
    "                        rel_weight_diffs = np.nan_to_num(rel_weight_diffs, nan=0)\n",
    "\n",
    "                        curr_unisim_slice = np.histogram(theta_df[\"reco_muon_momentum\"].to_numpy(), weights=theta_df[\"net_weight\"].to_numpy()*rel_weight_diffs, bins=muon_momentum_bins)[0]\n",
    "                        unisim_reco_hist_dic[unisim_type][j] += list(curr_unisim_slice)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    if use_real_data:\n",
    "        pickle.dump((xs_cv_reco_hist, universe_reco_hists, unisim_reco_hist_dic), open(\"universes_v6_real.pkl\", \"wb\"))\n",
    "    elif use_nuwro_fake_data:\n",
    "        pickle.dump((xs_cv_reco_hist, universe_reco_hists, unisim_reco_hist_dic), open(\"universes_v6_nuwro.pkl\", \"wb\"))\n",
    "    elif use_genie_v2_fake_data:\n",
    "        pickle.dump((xs_cv_reco_hist, universe_reco_hists, unisim_reco_hist_dic), open(\"universes_v6_genie_v2.pkl\", \"wb\"))\n",
    "else:\n",
    "    if use_real_data:\n",
    "        xs_cv_reco_hist, universe_reco_hists, unisim_reco_hist_dic = pickle.load(open(\"universes_v6_real.pkl\", \"rb\"))\n",
    "    elif use_nuwro_fake_data:\n",
    "        xs_cv_reco_hist, universe_reco_hists, unisim_reco_hist_dic = pickle.load(open(\"universes_v6_nuwro.pkl\", \"rb\"))\n",
    "    elif use_genie_v2_fake_data:\n",
    "        xs_cv_reco_hist, universe_reco_hists, unisim_reco_hist_dic = pickle.load(open(\"universes_v6_genie_v2.pkl\", \"rb\"))\n",
    "    \n",
    "uncollapsed_dim = len(universe_reco_hists[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if collapse_2d:\n",
    "\n",
    "    # collapsing everything to muon momentum and muon angle\n",
    "    # Combining FC/PC and Enu bins\n",
    "\n",
    "    collapsed_reco_hist = np.zeros(16*9)\n",
    "    collapsed_tot_pred = np.zeros(16*9)\n",
    "    collapsed_data = np.zeros(16*9)\n",
    "\n",
    "    for i in range(1152):\n",
    "        collapsed_reco_hist[i%(16*9)] += reco_hist[i]\n",
    "        collapsed_tot_pred[i%(16*9)] += tot_pred[i]\n",
    "        collapsed_data[i%(16*9)] += data[i]\n",
    "\n",
    "    collapsed_universe_reco_hists = []\n",
    "    for uni_i in range(600):\n",
    "        vals = np.zeros(16*9)\n",
    "        for i in range(1152):\n",
    "            vals[i%(16*9)] += universe_reco_hists[uni_i][i]\n",
    "        collapsed_universe_reco_hists.append(vals)\n",
    "\n",
    "    collapsed_unisim_reco_hist_dic = {}\n",
    "    for k, v in unisim_reco_hist_dic.items():\n",
    "        if k not in collapsed_unisim_reco_hist_dic:\n",
    "            collapsed_unisim_reco_hist_dic[k] = []\n",
    "        for uni_i in range(len(v)):\n",
    "            vals = np.zeros(16*9)\n",
    "            for i in range(1152):\n",
    "                vals[i%(16*9)] += v[uni_i][i]\n",
    "            collapsed_unisim_reco_hist_dic[k].append(vals)\n",
    "\n",
    "\n",
    "    reco_hist = collapsed_reco_hist\n",
    "    tot_pred = collapsed_tot_pred\n",
    "    data = collapsed_data\n",
    "\n",
    "    universe_reco_hists = collapsed_universe_reco_hists\n",
    "    unisim_reco_hist_dic = collapsed_unisim_reco_hist_dic\n",
    "\n",
    "elif collapse_1d:\n",
    "\n",
    "    # collapsing everything to muon momentum\n",
    "    # Combining FC/PC, Enu, and theta bins\n",
    "\n",
    "    collapsed_reco_hist = np.zeros(16)\n",
    "    collapsed_tot_pred = np.zeros(16)\n",
    "    collapsed_data = np.zeros(16)\n",
    "\n",
    "    for i in range(1152):\n",
    "        collapsed_reco_hist[i%16] += reco_hist[i]\n",
    "        collapsed_tot_pred[i%16] += tot_pred[i]\n",
    "        collapsed_data[i%16] += data[i]\n",
    "\n",
    "    collapsed_universe_reco_hists = []\n",
    "    for uni_i in range(600):\n",
    "        vals = np.zeros(16)\n",
    "        for i in range(1152):\n",
    "            vals[i%16] += universe_reco_hists[uni_i][i]\n",
    "        collapsed_universe_reco_hists.append(vals)\n",
    "\n",
    "    collapsed_unisim_reco_hist_dic = {}\n",
    "    for k, v in unisim_reco_hist_dic.items():\n",
    "        if k not in collapsed_unisim_reco_hist_dic:\n",
    "            collapsed_unisim_reco_hist_dic[k] = []\n",
    "        for uni_i in range(len(v)):\n",
    "            vals = np.zeros(16)\n",
    "            for i in range(1152):\n",
    "                vals[i%16] += v[uni_i][i]\n",
    "            collapsed_unisim_reco_hist_dic[k].append(vals)\n",
    "\n",
    "    reco_hist = collapsed_reco_hist\n",
    "    tot_pred = collapsed_tot_pred\n",
    "    data = collapsed_data\n",
    "\n",
    "    universe_reco_hists = collapsed_universe_reco_hists\n",
    "    unisim_reco_hist_dic = collapsed_unisim_reco_hist_dic\n",
    "\n",
    "\n",
    "collapsed_dim = len(universe_reco_hists[0])\n",
    "collapsed_plus_dim = collapsed_dim + 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MA_values = np.genfromtxt(\"knob_values/MaCCQE_univs.txt\")\n",
    "MEC_values = np.genfromtxt(\"knob_values/NormCCMEC_univs_v2.txt\")\n",
    "lambda_values = [np.sum(universe_reco_hists[i]) / np.sum(reco_hist) for i in range(600)]\n",
    "\n",
    "print(np.mean(MA_values), np.std(MA_values))\n",
    "print(np.mean(MEC_values), np.std(MEC_values))\n",
    "print(np.mean(lambda_values), np.std(lambda_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_pred_MA = list(tot_pred) + [1.10, 1.66, 1]\n",
    "\n",
    "\n",
    "universe_reco_MAs = []\n",
    "if shape_type == \"rate+shape\" or shape_type == \"+100\":\n",
    "    for i in range(600):\n",
    "        universe_reco_MAs.append(np.array(list(universe_reco_hists[i]) + [MA_values[i], MEC_values[i], lambda_values[i]]))\n",
    "elif shape_type == \"matrix_breakdown\":\n",
    "    for uni_i in range(600):\n",
    "        universe_reco_MAs.append(np.array(list(universe_reco_hists[uni_i] / np.sum(universe_reco_hists[uni_i])) + [MA_values[uni_i], MEC_values[i], lambda_values[uni_i]]))\n",
    "    not_normed_tot_pred_MA = tot_pred_MA.copy()\n",
    "    tot_pred_MA = list(tot_pred_MA[:-3] / np.sum(tot_pred_MA[:-3])) + [1.10, 1.66, 1]\n",
    "    data = data / np.sum(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = np.array(tot_pred_MA).shape[0]\n",
    "\n",
    "multisim_xs_MA_cov = np.zeros((dim, dim))\n",
    "\n",
    "for uni_i in tqdm(range(600)):\n",
    "    uni_reco_MA = universe_reco_MAs[uni_i]\n",
    "    row_diffs = np.tile(uni_reco_MA - tot_pred_MA, (dim, 1))\n",
    "    col_diffs = np.tile(np.reshape(uni_reco_MA - tot_pred_MA, (dim, 1)), (1, dim))\n",
    "    multisim_xs_MA_cov += row_diffs * col_diffs\n",
    "\n",
    "multisim_xs_MA_cov = multisim_xs_MA_cov / 600.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is fixing the fact that some of these two-length arrays contain the CV rather than a variation\n",
    "# so really we want to divide by one and not two in that case\n",
    "unisim_divide_number_dic = {\n",
    "    \"AxFFCCQEshape_UBGenie\": 1,\n",
    "    \"DecayAngMEC_UBGenie\": 1,\n",
    "    \"NormCCCOH_UBGenie\": 1,\n",
    "    \"NormNCCOH_UBGenie\": 1,\n",
    "    \"RPA_CCQE_UBGenie\": 2,\n",
    "    \"ThetaDelta2NRad_UBGenie\": 1,\n",
    "    \"Theta_Delta2Npi_UBGenie\": 1,\n",
    "    \"VecFFCCQEshape_UBGenie\": 1,\n",
    "    \"XSecShape_CCMEC_UBGenie\": 1,\n",
    "    \"xsr_scc_Fa3_SCC\": 10,\n",
    "    \"xsr_scc_Fv3_SCC\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding unisim variations as well\n",
    "\n",
    "unisim_xs_MA_cov = np.zeros((collapsed_plus_dim, collapsed_plus_dim))\n",
    "\n",
    "# pbar is inaccurate if skip_AxFFCCQEshape_UBGenie\n",
    "pbar = tqdm(total=np.sum(list(num_unisim_variations_dic.values())))\n",
    "\n",
    "if skip_AxFFCCQEshape_UBGenie:\n",
    "    unisim_types = [\"DecayAngMEC_UBGenie\",\n",
    "                    \"NormCCCOH_UBGenie\",\n",
    "                    \"NormNCCOH_UBGenie\",\n",
    "                    \"RPA_CCQE_UBGenie\",\n",
    "                    \"ThetaDelta2NRad_UBGenie\",\n",
    "                    \"Theta_Delta2Npi_UBGenie\",\n",
    "                    \"VecFFCCQEshape_UBGenie\",\n",
    "                    \"XSecShape_CCMEC_UBGenie\",\n",
    "                    \"xsr_scc_Fa3_SCC\",\n",
    "                    \"xsr_scc_Fv3_SCC\",]\n",
    "else:\n",
    "    unisim_types = [\"AxFFCCQEshape_UBGenie\",\n",
    "                    \"DecayAngMEC_UBGenie\",\n",
    "                    \"NormCCCOH_UBGenie\",\n",
    "                    \"NormNCCOH_UBGenie\",\n",
    "                    \"RPA_CCQE_UBGenie\",\n",
    "                    \"ThetaDelta2NRad_UBGenie\",\n",
    "                    \"Theta_Delta2Npi_UBGenie\",\n",
    "                    \"VecFFCCQEshape_UBGenie\",\n",
    "                    \"XSecShape_CCMEC_UBGenie\",\n",
    "                    \"xsr_scc_Fa3_SCC\",\n",
    "                    \"xsr_scc_Fv3_SCC\",]\n",
    "\n",
    "unisim_diag_errs = {}\n",
    "\n",
    "for unisim_type in unisim_types:\n",
    "\n",
    "    curr_unisim_xs_MA_cov = np.zeros((collapsed_plus_dim, collapsed_plus_dim))\n",
    "\n",
    "    for j in range(num_unisim_variations_dic[unisim_type]):\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "        diff = np.array(unisim_reco_hist_dic[unisim_type][j]) - np.array(reco_hist)\n",
    "        diff = np.append(diff, np.array([0, 0, 0]))\n",
    "\n",
    "        row_diffs = np.tile(diff, (collapsed_plus_dim, 1))\n",
    "        col_diffs = np.tile(np.reshape(diff, (collapsed_plus_dim, 1)), (1, collapsed_plus_dim))\n",
    "        curr_unisim_xs_MA_cov += row_diffs * col_diffs\n",
    "        \n",
    "    #curr_unisim_xs_MA_cov = curr_unisim_xs_MA_cov / num_unisim_variations_dic[unisim_type]\n",
    "    curr_unisim_xs_MA_cov = curr_unisim_xs_MA_cov / unisim_divide_number_dic[unisim_type]\n",
    "    unisim_xs_MA_cov += curr_unisim_xs_MA_cov\n",
    "\n",
    "    unisim_diag_errs[unisim_type] = np.sqrt(np.diag(curr_unisim_xs_MA_cov))\n",
    "\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "xs_MA_cov = multisim_xs_MA_cov + unisim_xs_MA_cov\n",
    "\n",
    "total_unisim_diag_errs = np.sqrt(np.diag(unisim_xs_MA_cov))\n",
    "total_multi_diag_errs = np.sqrt(np.diag(multisim_xs_MA_cov))\n",
    "\n",
    "total_xs_diag_errs = np.sqrt(np.diag(xs_MA_cov))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for unisim_type in unisim_types:\n",
    "    plt.plot(unisim_diag_errs[unisim_type][:-3] / tot_pred, label=unisim_type, ls=\"--\")\n",
    "plt.plot(total_unisim_diag_errs[:-3] / tot_pred, label=\"total unisim\", c=\"k\", lw=2)\n",
    "plt.ylim(0, 0.5)\n",
    "plt.xlim(0, 50)\n",
    "plt.xlabel(\"Bin Number\")\n",
    "plt.ylabel(\"Fractional Uncertainty\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(total_multi_diag_errs[:-3] / tot_pred, label=\"total multisim\", c=\"k\", lw=2)\n",
    "plt.ylim(0, 0.5)\n",
    "plt.xlim(0, 50)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Bin Number\")\n",
    "plt.ylabel(\"Fractional Uncertainty\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(total_xs_diag_errs[:-3] / tot_pred, label=\"total XS\")\n",
    "cov_17_from_london_frac_errs = np.sqrt(np.diag(cov_17_arr_from_london)) / cov_17_vec_mean_from_london\n",
    "plt.plot(cov_17_from_london_frac_errs, label=\"total XS from London's cov_17.root\")\n",
    "plt.ylim(0, 0.5)\n",
    "plt.xlim(0, 50)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Bin Number\")\n",
    "plt.ylabel(\"Fractional Uncertainty\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (collapse_2d or collapse_1d):\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot((total_xs_diag_errs[:-3] / tot_pred) / (np.sqrt(np.diag(cov_17_arr_from_london)) / cov_17_vec_mean_from_london) - 1, label=\"recalc_xs_frac_err / london_framework_xs_frac_err - 1\")\n",
    "    plt.plot(np.array(tot_pred) / 2500., label=\"tot_pred, arbitrary norm\")\n",
    "    plt.axhline(0, c=\"k\", ls=\"--\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Bin Number\")\n",
    "    plt.ylim(-2, 2)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot((total_xs_diag_errs[:-3] / tot_pred) / (np.sqrt(np.diag(cov_17_arr_from_london)) / cov_17_vec_mean_from_london) - 1, label=\"recalc_xs_frac_err / london_framework_xs_frac_err - 1\")\n",
    "    plt.plot(np.array(tot_pred) / 2500., label=\"tot_pred, arbitrary norm\")\n",
    "    plt.axhline(0, c=\"k\", ls=\"--\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Bin Number\")\n",
    "    plt.ylim(-2, 2)\n",
    "    plt.xlim(0, 200)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Pearson data stat uncertainty\n",
    "pearson_data_stat_cov_matrix = np.zeros((collapsed_dim, collapsed_dim))\n",
    "for i in range(collapsed_dim):\n",
    "    for j in range(collapsed_dim):\n",
    "        if i == j:\n",
    "            pearson_data_stat_cov_matrix[i][j] = tot_pred[i]\n",
    "cov_data_stat_new = pearson_data_stat_cov_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_stat = f_wiener[\"hcov_stat\"].to_numpy()[0]\n",
    "cov_mcstat = f_wiener[\"hcov_mcstat\"].to_numpy()[0]\n",
    "cov_add = f_wiener[\"hcov_add\"].to_numpy()[0]\n",
    "cov_det = f_wiener[\"hcov_det\"].to_numpy()[0]\n",
    "cov_flux = f_wiener[\"hcov_flux\"].to_numpy()[0]\n",
    "#cov_xs = f_wiener[\"hcov_xs\"].to_numpy()[0]\n",
    "#cov_tot = f_wiener[\"hcov_tot\"].to_numpy()[0]\n",
    "\n",
    "cov_stat.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if collapse_2d:\n",
    "\n",
    "    collapsing_matrix = [[1] + [0 for _ in range(16*9-1)]]\n",
    "    for i in range(1152):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        collapsing_matrix.append([0 for _ in range(i%(16*9))] + [1] + [0 for _ in range(16*9 - i%(16*9) - 1)])\n",
    "    collapsing_matrix = np.array(collapsing_matrix)\n",
    "\n",
    "    #cov_stat = np.linalg.multi_dot([np.transpose(collapsing_matrix), cov_stat, collapsing_matrix])\n",
    "    cov_mcstat = np.linalg.multi_dot([np.transpose(collapsing_matrix), cov_mcstat, collapsing_matrix])\n",
    "    cov_add = np.linalg.multi_dot([np.transpose(collapsing_matrix), cov_add, collapsing_matrix])\n",
    "    cov_det = np.linalg.multi_dot([np.transpose(collapsing_matrix), cov_det, collapsing_matrix])\n",
    "    cov_flux = np.linalg.multi_dot([np.transpose(collapsing_matrix), cov_flux, collapsing_matrix])\n",
    "\n",
    "    cov_stat_MA = np.append(np.append(cov_data_stat_new, np.zeros((3, collapsed_dim)), axis=0), np.zeros((collapsed_dim+3,3)), axis=1)\n",
    "    cov_mcstat_MA = np.append(np.append(cov_mcstat, np.zeros((3, collapsed_dim)), axis=0), np.zeros((collapsed_dim+3,3)), axis=1)\n",
    "    cov_add_MA = np.append(np.append(cov_add, np.zeros((3, collapsed_dim)), axis=0), np.zeros((collapsed_dim+3,3)), axis=1)\n",
    "    cov_det_MA = np.append(np.append(cov_det, np.zeros((3, collapsed_dim)), axis=0), np.zeros((collapsed_dim+3,3)), axis=1)\n",
    "    cov_flux_MA = np.append(np.append(cov_flux, np.zeros((3, collapsed_dim)), axis=0), np.zeros((collapsed_dim+3,3)), axis=1)\n",
    "\n",
    "\n",
    "elif collapse_1d:\n",
    "\n",
    "    collapsing_matrix = [[1] + [0 for _ in range(16-1)]]\n",
    "    for i in range(1152):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        collapsing_matrix.append([0 for _ in range(i%16)] + [1] + [0 for _ in range(16 - i%16 - 1)])\n",
    "    collapsing_matrix = np.array(collapsing_matrix)\n",
    "\n",
    "    #cov_stat = np.linalg.multi_dot([np.transpose(collapsing_matrix), cov_stat, collapsing_matrix])\n",
    "    cov_mcstat = np.linalg.multi_dot([np.transpose(collapsing_matrix), cov_mcstat, collapsing_matrix])\n",
    "    cov_add = np.linalg.multi_dot([np.transpose(collapsing_matrix), cov_add, collapsing_matrix])\n",
    "    cov_det = np.linalg.multi_dot([np.transpose(collapsing_matrix), cov_det, collapsing_matrix])\n",
    "    cov_flux = np.linalg.multi_dot([np.transpose(collapsing_matrix), cov_flux, collapsing_matrix])\n",
    "\n",
    "    cov_stat_MA = np.append(np.append(cov_data_stat_new, np.zeros((3, collapsed_dim)), axis=0), np.zeros((collapsed_dim+3,3)), axis=1)\n",
    "    cov_mcstat_MA = np.append(np.append(cov_mcstat, np.zeros((3, collapsed_dim)), axis=0), np.zeros((collapsed_dim+3,3)), axis=1)\n",
    "    cov_add_MA = np.append(np.append(cov_add, np.zeros((3, collapsed_dim)), axis=0), np.zeros((collapsed_dim+3,3)), axis=1)\n",
    "    cov_det_MA = np.append(np.append(cov_det, np.zeros((3, collapsed_dim)), axis=0), np.zeros((collapsed_dim+3,3)), axis=1)\n",
    "    cov_flux_MA = np.append(np.append(cov_flux, np.zeros((3, collapsed_dim)), axis=0), np.zeros((collapsed_dim+3,3)), axis=1)\n",
    "\n",
    "else:\n",
    "    cov_stat_MA = np.append(np.append(cov_data_stat_new, np.zeros((3, collapsed_dim)), axis=0), np.zeros((collapsed_dim+3,3)), axis=1)\n",
    "    cov_mcstat_MA = np.append(np.append(cov_mcstat, np.zeros((3, collapsed_dim)), axis=0), np.zeros((collapsed_dim+3,3)), axis=1)\n",
    "    cov_add_MA = np.append(np.append(cov_add, np.zeros((3, collapsed_dim)), axis=0), np.zeros((collapsed_dim+3,3)), axis=1)\n",
    "    cov_det_MA = np.append(np.append(cov_det, np.zeros((3, collapsed_dim)), axis=0), np.zeros((collapsed_dim+3,3)), axis=1)\n",
    "    cov_flux_MA = np.append(np.append(cov_flux, np.zeros((3, collapsed_dim)), axis=0), np.zeros((collapsed_dim+3,3)), axis=1)\n",
    "\n",
    "total_cov_MA = (xs_MA_cov\n",
    "        + cov_stat_MA\n",
    "        + cov_mcstat_MA\n",
    "        + cov_add_MA\n",
    "        + cov_det_MA\n",
    "        + cov_flux_MA\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = total_cov_MA.shape[0]\n",
    "\n",
    "if shape_type == \"+100\":\n",
    "\n",
    "    percent_normalization_error = 100.\n",
    "\n",
    "    dim = 1152\n",
    "\n",
    "    row_diffs = np.tile(tot_pred, (dim, 1))\n",
    "    col_diffs = np.tile(np.reshape(tot_pred, (dim, 1)), (1, dim))\n",
    "    extra_normalization_cov = row_diffs * col_diffs\n",
    "    extra_normalization_cov = np.append(\n",
    "        np.append(\n",
    "            extra_normalization_cov, np.zeros((3, dim)),\n",
    "        axis=0), \n",
    "        np.zeros((dim+3,3))\n",
    "    , axis=1)\n",
    "\n",
    "    total_cov_MA += extra_normalization_cov * percent_normalization_error**2 / (100. * 100.)\n",
    "\n",
    "elif shape_type == \"matrix_breakdown\":\n",
    "\n",
    "    # non xs cov_MA\n",
    "    M = (cov_stat_MA\n",
    "       + cov_mcstat_MA\n",
    "       + cov_add_MA\n",
    "       + cov_det_MA\n",
    "       + cov_flux_MA)\n",
    "\n",
    "\n",
    "    # from docDB 5926\n",
    "\n",
    "    M_s = np.zeros((n, n))\n",
    "    M_n = np.zeros((n, n))\n",
    "    M_m = np.zeros((n, n))\n",
    "\n",
    "    N = np.array(not_normed_tot_pred_MA)\n",
    "    N_T = np.sum(N)\n",
    "    row_sums = [np.sum(M[i, :]) for i in range(n)]\n",
    "    normalized_N = N / N_T\n",
    "    M_sum = np.sum(M)\n",
    "\n",
    "    row_sum_terms = []\n",
    "    matrix_sum_terms = []\n",
    "    normalized_N_squared = []\n",
    "\n",
    "    print(\"extracting non-XS covariance matrix normalization component...\")\n",
    "    for i in tqdm(range(n)):\n",
    "        for j in range(n):\n",
    "\n",
    "            M_s[i][j] = (M[i][j]\n",
    "                 - normalized_N[j] * row_sums[i]\n",
    "                 - normalized_N[i] * row_sums[j]\n",
    "                 + normalized_N[i] * normalized_N[j] * M_sum\n",
    "                ) / (N_T * N_T)\n",
    "\n",
    "            M_m[i][j] = (normalized_N[j] * row_sums[i]\n",
    "                 + normalized_N[i] * row_sums[j]\n",
    "                 - 2. * normalized_N[i] * normalized_N[j] * M_sum\n",
    "                ) / (N_T * N_T)\n",
    "\n",
    "            M_n[i][j] = normalized_N[i] * normalized_N[j] * M_sum / (N_T * N_T)\n",
    "\n",
    "\n",
    "    total_cov_MA = M_s + xs_MA_cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trio_prior = [1.1, 1.66, 1.]\n",
    "\n",
    "cov_cross = total_cov_MA[-3:, :-3]\n",
    "cov_constraining = total_cov_MA[:-3, :-3]\n",
    "cov_prior = total_cov_MA[-3:, -3:]\n",
    "inv_cov_constraining = np.linalg.inv(cov_constraining)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"prior M_A: \", trio_prior[0], \"+/-\", np.sqrt(cov_prior[0][0]))\n",
    "print(\"prior NormCCMEC: \", trio_prior[1], \"+/-\", np.sqrt(cov_prior[1][1]))\n",
    "print(\"prior lambda: \", trio_prior[2], \"+/-\", np.sqrt(cov_prior[2][2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Real Or Fake Data Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_constrained_trio = tot_pred_MA[-3:] + np.linalg.multi_dot(\n",
    "    [cov_cross, inv_cov_constraining, np.array(data) - np.array(tot_pred_MA[:-3])]\n",
    ")\n",
    "data_constrained_trio_cov = total_cov_MA[-3:,-3:] - np.linalg.multi_dot(\n",
    "    [cov_cross, inv_cov_constraining, np.transpose(cov_cross)]\n",
    ")\n",
    "\n",
    "\n",
    "print(\"constrained M_A: \", data_constrained_trio[0], \"+/-\", np.sqrt(data_constrained_trio_cov[0][0]))\n",
    "print(\"constrained NormCCMEC: \", data_constrained_trio[1], \"+/-\", np.sqrt(data_constrained_trio_cov[1][1]))\n",
    "print(\"constrained lambda: \", data_constrained_trio[2], \"+/-\", np.sqrt(data_constrained_trio_cov[2][2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Asimov Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = np.array(tot_pred_MA[:-3])\n",
    "\n",
    "asimov_constrained_trio = tot_pred_MA[-3:] + np.linalg.multi_dot(\n",
    "    [cov_cross, inv_cov_constraining, fake_data - np.array(tot_pred_MA[:-3])]\n",
    ")\n",
    "asimov_constrained_trio_cov = total_cov_MA[-3:,-3:] - np.linalg.multi_dot(\n",
    "    [cov_cross, inv_cov_constraining, np.transpose(cov_cross)]\n",
    ")\n",
    "\n",
    "print(\"constrained M_A: \", asimov_constrained_trio[0], \"+/-\", np.sqrt(asimov_constrained_trio_cov[0][0]))\n",
    "print(\"constrained NormCCMEC: \", asimov_constrained_trio[1], \"+/-\", np.sqrt(asimov_constrained_trio_cov[1][1]))\n",
    "print(\"constrained lambda: \", asimov_constrained_trio[2], \"+/-\", np.sqrt(asimov_constrained_trio_cov[2][2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Fake Data Samples From Cov Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_datas_reco_MAs = np.random.multivariate_normal(tot_pred_MA, total_cov_MA, size=600)\n",
    "\n",
    "cov_mat_trios = []\n",
    "cov_mat_trio_covs = []\n",
    "cov_mat_true_trios = []\n",
    "\n",
    "for uni_i in tqdm(range(600)):\n",
    "\n",
    "    # CV with cov matrix variations\n",
    "    fake_data = fake_datas_reco_MAs[uni_i][:-3]\n",
    "    \n",
    "\n",
    "    fake_constrained_trio = tot_pred_MA[-3:] + np.linalg.multi_dot(\n",
    "        [cov_cross, inv_cov_constraining, fake_data - np.array(tot_pred_MA[:-3])]\n",
    "    )\n",
    "    fake_constrained_trio_cov = total_cov_MA[-3:,-3:] - np.linalg.multi_dot(\n",
    "        [cov_cross, inv_cov_constraining, np.transpose(cov_cross)]\n",
    "    )\n",
    "\n",
    "    cov_mat_trios.append(fake_constrained_trio)\n",
    "    cov_mat_trio_covs.append(fake_constrained_trio_cov)\n",
    "    cov_mat_true_trios.append(fake_datas_reco_MAs[uni_i][-3:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Fake Data Samples From XS Cov Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_datas_reco_MAs = np.random.multivariate_normal(tot_pred_MA, xs_MA_cov, size=600)\n",
    "\n",
    "xs_cov_mat_trios = []\n",
    "xs_cov_mat_trio_covs = []\n",
    "xs_cov_mat_true_trios = []\n",
    "\n",
    "for uni_i in tqdm(range(600)):\n",
    "\n",
    "    # CV with cov matrix variations\n",
    "    fake_data = fake_datas_reco_MAs[uni_i][:-3]\n",
    "\n",
    "    fake_constrained_trio = tot_pred_MA[-3:] + np.linalg.multi_dot(\n",
    "        [cov_cross, inv_cov_constraining, fake_data - np.array(tot_pred_MA[:-3])]\n",
    "    )\n",
    "    fake_constrained_trio_cov = total_cov_MA[-3:,-3:] - np.linalg.multi_dot(\n",
    "        [cov_cross, inv_cov_constraining, np.transpose(cov_cross)]\n",
    "    )\n",
    "\n",
    "    xs_cov_mat_trios.append(fake_constrained_trio)\n",
    "    xs_cov_mat_trio_covs.append(fake_constrained_trio_cov)\n",
    "    xs_cov_mat_true_trios.append(fake_datas_reco_MAs[uni_i][-3:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Fake Data Samples From GENIE Universes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genie_trios = []\n",
    "genie_trio_covs = []\n",
    "genie_true_trios = []\n",
    "\n",
    "for uni_i in tqdm(range(600)):\n",
    "\n",
    "    # CV with GENIE variations\n",
    "    fake_data = universe_reco_MAs[uni_i][:-3]\n",
    "\n",
    "    fake_constrained_trio = tot_pred_MA[-3:] + np.linalg.multi_dot(\n",
    "        [cov_cross, inv_cov_constraining, fake_data - np.array(tot_pred_MA[:-3])]\n",
    "    )\n",
    "    fake_constrained_trio_cov = total_cov_MA[-3:,-3:] - np.linalg.multi_dot(\n",
    "        [cov_cross, inv_cov_constraining, np.transpose(cov_cross)]\n",
    "    )\n",
    "\n",
    "    genie_trios.append(fake_constrained_trio)\n",
    "    genie_trio_covs.append(fake_constrained_trio_cov)\n",
    "    genie_true_trios.append(universe_reco_MAs[uni_i][-3:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving To Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"trio_pickles/\"\n",
    "\n",
    "if use_real_data:\n",
    "    name += \"real\"\n",
    "elif use_nuwro_fake_data:\n",
    "    name += \"nuwro_fake\"\n",
    "elif use_genie_v2_fake_data:\n",
    "    name += \"genie_v2_fake\"\n",
    "\n",
    "name += \"_\" + shape_type\n",
    "\n",
    "if collapse_2d:\n",
    "    name += \"_2d\"\n",
    "elif collapse_1d:\n",
    "    name += \"_1d\"\n",
    "else:\n",
    "    name += \"_3d\"\n",
    "\n",
    "with open(f'{name}.pkl', 'wb') as handle:\n",
    "    tup = (\n",
    "\n",
    "        trio_prior, cov_prior,\n",
    "\n",
    "        data_constrained_trio, data_constrained_trio_cov,\n",
    "        asimov_constrained_trio, asimov_constrained_trio_cov,\n",
    "        \n",
    "        cov_mat_true_trios, cov_mat_trios, cov_mat_trio_covs,\n",
    "        xs_cov_mat_true_trios, xs_cov_mat_trios, xs_cov_mat_trio_covs,\n",
    "        genie_true_trios, genie_trios, genie_trio_covs\n",
    "    )\n",
    "\n",
    "    pickle.dump(tup, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
